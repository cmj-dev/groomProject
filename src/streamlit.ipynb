{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#설치"
      ],
      "metadata": {
        "id": "iAXWHjNW4PP5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3FXb6bp5egJ",
        "outputId": "79fec893-a49f-431a-a7cc-5dd5b9462385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 9.2 MB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 87.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 53.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 9.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 164 kB 94.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 90.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.3 MB/s \n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "kBvuOB2157ar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2819ea-a66b-4bdf-fdab-36a0b535c748"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=326bd7d3a3f204d23e2593035d8c5b32fe5c1ebcdb58118579f646b3b2e1a8aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ngrok에서 토큰 받아오기\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token('ngrok에서 토큰')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB9y7VcEHigH",
        "outputId": "573e5036-a412-4230-8d5a-ce4cf27836a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#modelLoader.py와 generaterMethod.py가 업로드되어야만 사용가능합니다\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "rOdndRStBp-8",
        "outputId": "cfad2a3e-b238-480c-fed8-14e56b69dd1d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7bfa6669-7778-4295-ba41-ff295f728e77\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7bfa6669-7778-4295-ba41-ff295f728e77\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving modelLoader.py to modelLoader.py\n",
            "Saving generaterMethod.py to generaterMethod.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'modelLoader.py': b\"import wandb\\nimport torch\\nimport gdown\\nfrom transformers import(\\n    AutoModelForSequenceClassification,\\n    AutoModelForCausalLM,\\n    AutoTokenizer\\n)\\n\\ndef loadModelForSequenceClassification(model_name, artifact_version, model_path = '/best_model_at_end/pytorch_model.bin'):\\n    run = wandb.init()\\n    artifact = run.use_artifact(artifact_version, type='model')\\n    artifact_dir = artifact.download()\\n\\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\\n    model.load_state_dict(torch.load(artifact_dir+model_path))\\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\\n    return (model, tokenizer)\\n\\ndef gdLoadModelForSequenceClassification(model_name, artifact_version = '1IBoQCiD9cW9D7AmcE3BbLDG6TRm9zA02'):\\n    url = 'https://drive.google.com/uc?id=' + artifact_version\\n    artifact_dir = 'modelSequenceClassification.bin'\\n\\n    gdown.download(url, artifact_dir, quiet=False)\\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\\n    model.load_state_dict(torch.load(artifact_dir))\\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\\n    return (model, tokenizer)\\n\\ndef loadModelForCausalLM(model_name, artifact_version, model_path = '/best_model_at_end/pytorch_model.bin'):\\n    run = wandb.init()\\n    artifact = run.use_artifact(artifact_version, type='model')\\n    artifact_dir = artifact.download()\\n\\n    model = AutoModelForCausalLM.from_pretrained(model_name)\\n    model.load_state_dict(torch.load(artifact_dir+model_path))\\n    tokenizer = AutoTokenizer.from_pretrained(model_name, \\n                                              bos_token='</s>', eos_token='</s>', unk_token='<unk>',\\n                                              pad_token='<pad>', mask_token='<mask>')\\n    return (model, tokenizer)\\n\\ndef gdLoadModelForCausalLM(model_name, artifact_version = '1_PJoPgx0gsE2HrXdXmcW-uhPh5R3CgxC'):\\n    url = 'https://drive.google.com/uc?id=' + artifact_version\\n    artifact_dir = 'modelCausalLM.bin'\\n\\n    gdown.download(url, artifact_dir, quiet=False)\\n    model = AutoModelForCausalLM.from_pretrained(model_name)\\n    model.load_state_dict(torch.load(artifact_dir))\\n    tokenizer = AutoTokenizer.from_pretrained(model_name, \\n                                              bos_token='</s>', eos_token='</s>', unk_token='<unk>',\\n                                              pad_token='<pad>', mask_token='<mask>')\\n    return (model, tokenizer)\",\n",
              " 'generaterMethod.py': b'import torch, re\\nfrom tqdm import tqdm as tqdm\\nfrom datasets import Dataset\\nfrom datasets.utils import disable_progress_bar\\nfrom itertools import combinations\\nfrom transformers import DefaultDataCollator\\n\\nclass generater:\\n    def __init__(self, model, tokenizer, gen_model, gen_tokenizer, batch_size = 64):\\n        self.device = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n        self.model = model.to(self.device)\\n        self.tokenizer = tokenizer\\n        self.gen_model = gen_model\\n        self.gen_tokenizer = gen_tokenizer\\n        self.batch_size = batch_size\\n\\n    def tokenizeWithoutLabel(self, data):\\n        max_length = 128\\n        tokenized_datas = self.tokenizer(\\n            data[\\'texts\\'],\\n            max_length=max_length,\\n            padding=\"max_length\",\\n            truncation=\"only_second\"\\n        )\\n        return tokenized_datas\\n\\n    def deleteListIndex(self, tokens, indexs):\\n        out = tokens[:]\\n        indexs = list(indexs)\\n        for index in indexs[::-1]:\\n            if out[index] == \\'[UNK]\\':\\n                continue\\n            del out[index]\\n        return out\\n\\n    def deleteStyleToken(self, text, batch_size):\\n        disable_progress_bar()\\n        if torch.cuda.is_available():\\n            torch.cuda.empty_cache()\\n        texts = []\\n        texts.append({\\'ids\\': [0], \\'texts\\' : text})\\n        tokens = self.tokenizer.encode(text)\\n        token_indexs = range(1, len(tokens) - 1)\\n        for n in range(1,4):\\n            if len(token_indexs) < n - 1:\\n                continue\\n            for indexs in combinations(token_indexs, n):\\n                texts.append({\\'ids\\' : indexs, \\'texts\\' : self.tokenizer.decode(self.deleteListIndex(tokens, indexs)[1:-1])})\\n        line_data = Dataset.from_list(texts)\\n        line_tokenized_datasets = line_data.map(self.tokenizeWithoutLabel, batched=True, remove_columns=line_data.column_names)\\n        data_collator = DefaultDataCollator(return_tensors=\"pt\")\\n        line_loader = torch.utils.data.DataLoader(line_tokenized_datasets, batch_size=batch_size,\\n                                                shuffle=False, collate_fn=data_collator,\\n                                                num_workers=0)\\n        max_diff = 0\\n        max_info = {}\\n        vanil_prob = 0\\n        for up_i, id_inputs in enumerate(line_loader):\\n            inputs = {}\\n            inputs[\\'input_ids\\'] = id_inputs[\\'input_ids\\'].to(self.device)\\n            inputs[\\'token_type_ids\\'] = id_inputs[\\'token_type_ids\\'].to(self.device)\\n            inputs[\\'attention_mask\\'] = id_inputs[\\'attention_mask\\'].to(self.device)\\n            outputs = self.model(**inputs)\\n            logits = outputs.logits\\n            softmax = logits.softmax(dim=-1)\\n            for i, prob in enumerate(softmax):\\n                if up_i == 0 and i == 0:\\n                    if prob[1] < 0.7:\\n                        return \\n                    else:\\n                        vanil_prob = prob\\n                else:\\n                    if vanil_prob[1] - prob[1] > max_diff:\\n                        max_info = (texts[up_i*batch_size + i], prob)\\n                        max_diff = vanil_prob[1] - prob[1]\\n        if max_diff >= 0.2:\\n            return {\\'masked\\':max_info[0][\\'texts\\'], \\'original\\':text}\\n            \\n    def tokenizeMasked(self, data):\\n        text = self.deleteStyleToken(data, self.batch_size)\\n        tokenized_datas = self.gen_tokenizer(\\n            f\"<unused0> <unused1> {text[\\'masked\\']} <unused2>\",\\n            return_tensors=\"pt\"\\n        )\\n        return tokenized_datas\\n\\n    def makeMoralText(self, immoral_text):\\n        input = self.tokenizeMasked(immoral_text)\\n        gen_ids = self.gen_model.generate(**input,\\n                                max_length=256,\\n                                pad_token_id=self.gen_tokenizer.pad_token_id,\\n                                eos_token_id=self.gen_tokenizer.eos_token_id,\\n                                bos_token_id=self.gen_tokenizer.bos_token_id)\\n        output = self.gen_tokenizer.decode(gen_ids[0])\\n        pred = re.search(\\'2\\\\>\\\\s(.+?)\\\\s\\\\<u\\', output)\\n        ans=pred.group(1)\\n        return ans'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install import_ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Uk4YYXmCvsG",
        "outputId": "c2968272-22db-4366-fe0e-abd54caf9c32"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting import_ipynb\n",
            "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import_ipynb) (5.7.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import_ipynb) (7.9.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (2.0.10)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->IPython->import_ipynb) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import_ipynb) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import_ipynb) (1.15.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (2.16.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (4.13.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->import_ipynb) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->import_ipynb) (4.1.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (5.10.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import_ipynb) (0.7.0)\n",
            "Installing collected packages: jedi, import-ipynb\n",
            "Successfully installed import-ipynb-0.1.4 jedi-0.18.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#run"
      ],
      "metadata": {
        "id": "jRyvZTgPBdiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import generaterMethod\n",
        "import modelLoader\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "\n",
        "def main():\n",
        "    st.title(\"악플 순화기\")\n",
        "    input_comment=st.text_input('댓글을 입력해주세요')\n",
        "    check_button=st.button('확인')\n",
        "    \n",
        "    art1='groom2team/pj3_classifier_data_add/pytorch_finetuned:v2'\n",
        "    art1model='beomi/KcELECTRA-base-v2022'\n",
        "    art2model='skt/kogpt2-base-v2'\n",
        "    art2='groom2team/pj3_generater_gpt2/pytorch_finetuned:v1'\n",
        "\n",
        "    lmodel,ltokenizer=modelLoader.loadModelForSequenceClassification(art1model,art1)\n",
        "    gmodel,gtokenizer=modelLoader.loadModelForCausalLM(art2model,art2)\n",
        "\n",
        "    emodel=generaterMethod.generater(lmodel,ltokenizer,gmodel,gtokenizer)\n",
        "    \n",
        "    if check_button:\n",
        "        output=emodel.makeMoralText(input_comment)\n",
        "        st.text_area(\"결과\", value=output)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RFsQ8t0BmyB",
        "outputId": "552b76da-c48e-4f4b-ae81-79da4e8d0625"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model->classifier"
      ],
      "metadata": {
        "id": "tzYCYqwGWEnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HtuZfF-lbsj",
        "outputId": "f14e23b2-bb66-49ee-e4db-669bcf9159b3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\t   drive\t       genEval.ipynb   __pycache__  wandb\n",
            "artifacts  generaterMethod.py  modelLoader.py  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#서버연결\n",
        "!nohup streamlit run app.py --server.port 80 &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMFeXw8DlgXT",
        "outputId": "7e3eddad-8676-4173-8c27-5e353ac6b6ce"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url=ngrok.connect(port='80')\n",
        "url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTnNe5Gnlu-w",
        "outputId": "400bed08-f5ae-4c18-8f5c-4f822639c31e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://2bab-35-226-34-0.ngrok.io\" -> \"http://localhost:80\">"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#만료"
      ],
      "metadata": {
        "id": "hBL7gULCndbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkbQUJs3mq7a",
        "outputId": "9e2e4d71-72c5-4a29-a381-167b20a4ca8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process:Killing ngrok process: 565\n",
            "2022-10-31 05:55:36.471 Killing ngrok process: 565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kill 163"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pZa28bynR4g",
        "outputId": "5494e3f1-503b-4d13-958f-f76973f14cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: kill: (163) - No such process\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM1XT8SznCX1",
        "outputId": "d0b22088-629a-4aa5-d24b-3947c6d68ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    PID TTY          TIME CMD\n",
            "      1 ?        00:00:00 docker-init\n",
            "      7 ?        00:00:07 node\n",
            "     19 ?        00:00:01 tail\n",
            "     34 ?        00:00:00 run.sh\n",
            "     36 ?        00:00:00 kernel_manager_\n",
            "     47 ?        00:00:05 python3 <defunct>\n",
            "     48 ?        00:00:00 colab-fileshim.\n",
            "     61 ?        00:00:06 jupyter-noteboo\n",
            "     62 ?        00:00:06 dap_multiplexer\n",
            "     76 ?        00:00:38 python3\n",
            "     94 ?        00:00:13 python3\n",
            "    484 ?        00:00:44 node\n",
            "    546 ?        00:00:02 streamlit\n",
            "    640 ?        00:00:00 ps\n"
          ]
        }
      ]
    }
  ]
}